{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dbf9202-d5f0-44a7-b5cf-4a62fd95f422",
   "metadata": {},
   "source": [
    "# $$ Overview $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0d3b79-eca5-45e3-85d2-336b3f5b7edc",
   "metadata": {},
   "source": [
    "## What is an LLM\n",
    "A `Large Language Model` (LLM) is a neural network trained on massive amounts of text to **understand**, **generate**, and **manipulate** human language. <br/>\n",
    "* Large” refers to both model size (billions of parameters) and the scale of data used during training\n",
    "  \n",
    "### Why LLMs are so important?\n",
    "LLMs power modern applications such as *ChatGPT*, *translation systems*, *question answering*, *summarization*, and even *code generation*.\n",
    "\n",
    "\n",
    "**The Key Behind Their Success:**\n",
    "> Transformer architecture<br/>\n",
    "> Huge training datasets<br/>\n",
    "\n",
    "→ Together allow modeling of complex linguistic behavior that cannot be hand-coded\n",
    "\n",
    "**GenAI**:<br/>\n",
    "Because LLMs can generate text, they are categorized as `generative AI (GenAI)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1bb05f-f26f-464a-a1d8-0c68f8f46a6f",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d729d349-4c27-4a22-80d6-05453912bc28",
   "metadata": {},
   "source": [
    "### Limitations of Pre-LLM NLP\n",
    "Earlier <span style=\"color:RED\"> **NLP** </span> models were effective only in simple, narrow tasks but failed at deep understanding or generating coherent text.\n",
    "\n",
    "**LLMs** vs **Traditional Models**:\n",
    "- Traditional NLP → single-task systems.\n",
    "- LLMs → general-purpose, adaptable to many tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769a037e-434e-40c2-88a0-856a02899cd5",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; margin-top: 20px;\">\n",
    "  <img \n",
    "    src=\"https://raw.githubusercontent.com/salavii/llm-from-scratch/main/images/llm_hierarchy.png\"\n",
    "    style=\"width: 450px; border-radius: 10px; display: block; margin-left: auto; margin-right: auto;\"\n",
    "  >\n",
    "\n",
    "  <p style=\"font-size: 16px; color: #333; font-weight: bold; margin-top: 10px;\">\n",
    "    Relationship between AI → Machine Learning → Deep Learning → GenAI → LLMs <br/>\n",
    "    AI refers to systems that exhibit human-like intelligence.<br/>\n",
    "    ML learns patterns automatically from data.<br/>\n",
    "    Deep Learning uses multilayer neural networks.<br/>\n",
    "    GenAI involves the use of deep neural networks to create new content, such as text, images, or various forms of media.<br/>\n",
    "    LLMs are deep neural networks specialized for processing and generating human-like text.\n",
    "  </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4919ff9-31a7-4ca5-b99b-e34eef701d87",
   "metadata": {},
   "source": [
    "#### In short\n",
    "`LLMs` are invaluable for automating almost any task that involves parsing and generating text. <br/>\n",
    "Their applications are virtually endless, and as we continue to innovate and explore new ways to use these models, it’s clear that LLMs have the potential to redefine our relationship with technology, making it more conversational, intuitive, and accessible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c245dd79-7d0c-41ac-916a-8bd69298b228",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; margin-top: 20px;\">\n",
    "  <img \n",
    "    src=\"https://raw.githubusercontent.com/salavii/llm-from-scratch/main/images/llm_chat_example.png\"\n",
    "    style=\"width: 450px; border-radius: 10px; display: block; margin-left: auto; margin-right: auto;\"\n",
    "  >\n",
    "    <p style=\"font-size: 16px; color: #333; font-weight: bold; margin-top: 10px;\">\n",
    "   <b>Example of LLM Interaction:</b><br>\n",
    "        This example shows how an LLM can understand a casual user request and generate<br/>\n",
    "        a humorous, context-aware response.<br><br>\n",
    "        It demonstrates the model’s ability to interpret intent and produce natural,\n",
    "        creative text output.\n",
    "  </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c08500-cc93-4af0-9b6f-390d158e3cf1",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fefaac-86f5-4be9-871a-1ee74237a0bf",
   "metadata": {},
   "source": [
    "##  Why should we build our own LLMs? \n",
    "Building an LLM from scratch helps understand its internal mechanics and limitations. <br/>\n",
    "Also, it equips us with the required knowledge for `pretraining` or `fine-tuning` existing open source LLM architec tures to our own domain-specific datasets or tasks. \n",
    "- <span style=\"color:BLUE\"> **Pretraining**</span> provides general language understanding using large, diverse datasets.\n",
    "- <span style=\"color:BLUE\"> **Fine-tuning**</span> adapts the pretrained model to domain-specific tasks using smaller labeled datasets.\n",
    "\n",
    "**Custom LLMs can outperform general-purpose models on specialized tasks and offer several advantages**\n",
    "1. Improved privacy: companies may prefer not to share sensitive data with OpenAI due to confidentiality concerns.\n",
    "2. Lower latency (local deployment)\n",
    "3. Reduced server cost\n",
    "4. Full control over updates and behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82aaf12e-569e-4110-b6ac-05c83b885d27",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; margin-top: 20px;\">\n",
    "  <img \n",
    "    src=\"https://raw.githubusercontent.com/salavii/llm-from-scratch/main/images/llm_building_process.png\"\n",
    "    style=\"width: 450px; border-radius: 10px; display: block; margin-left: auto; margin-right: auto;\"\n",
    "  >\n",
    "\n",
    "  <p style=\"font-size: 16px; color: #333; font-weight: bold; margin-top: 10px;\">\n",
    "    This diagram shows how an LLM is built: first it learns general language patterns through pretraining,\n",
    "    then it is adapted to specific tasks through  fine-tuning\n",
    "  </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d6233c-9abe-439f-98db-b009eb17d44a",
   "metadata": {},
   "source": [
    "##  The first step in creating an LLM\n",
    "### Pretraining Overview\n",
    "- LLM training begins with a large corpus of unlabeled *raw text*.\n",
    "- The model learns using **self-supervised learning**, where the model generates its own labels from the input data.\n",
    "- The result is a **base (foundation) model** with core abilities such as next-word prediction, text completion, and limited few-shot learning.\n",
    "\n",
    "### Fine-tuning Overview\n",
    "- The pretrained model is then adapted using **labeled datasets** for specific tasks.\n",
    "- Two common fine-tuning approaches:\n",
    "  - **Instruction fine-tuning:** pairs of instructions and desired outputs (e.g., “translate this sentence → correct translation”).\n",
    "  - **Classification fine-tuning:** text samples paired with class labels (e.g., spam vs. not spam).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f74a636-f886-4cd5-94c1-7f75843cfbde",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27249cf6-d933-4949-9a1d-02d22e613709",
   "metadata": {},
   "source": [
    "# Transformer Architecture\n",
    "Modern LLMs are based on the transformer architecture.<br/>\n",
    "The architecture contains two main components:\n",
    "> **Encoder:** The encoder module processes the input text and encodes it into a series of numerical representations or vectors that capture the contextual information of the input. <br/> <br/> \n",
    ">**Decoder:**  The decoder module takes these encoded vectors and generates the out\n",
    "put text.\n",
    "\n",
    "For example, in a translation task, the encoder would encode the text from the source language into vectors, and the decoder would decode these vectors to generate text in the target language.\n",
    "Both the encoder and decoder consist of many layers connected by a so-called **self-attention** mechanism.\n",
    "\n",
    "- A key component is **self-attention**, which lets the model to weigh the importance of different words and capture long-range dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de534ab4-9241-42ab-9b20-2c744d35fc8c",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; margin-top: 20px;\">\n",
    "  <img \n",
    "    src=\"https://raw.githubusercontent.com/salavii/llm-from-scratch/main/images/llm-encoder-decoder.gif\n",
    "\"\n",
    "    style=\"width: 600px; border-radius: 10px; display: block; margin-left: auto; margin-right: auto;\"\n",
    "  >\n",
    "\n",
    "  <p style=\"font-size: 16px; color: #333; font-weight: bold; margin-top: 10px;\">\n",
    "    the encoder reads and understands the full input sentence, producing contextual embeddings, and the decoder uses this information to generate the   output sequence step by step\n",
    "  </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1394c94f-b386-4ff3-932c-9f3899e3c604",
   "metadata": {},
   "source": [
    "## BERT vs GPT (Transformer Variants)\n",
    "BERT and GPT are both based on the Transformer architecture.<br/>\n",
    "They adapt the original transformer for different NLP tasks\n",
    "\n",
    "- **BERT** (Encoder-only):\n",
    "  - Trained with masked word prediction (MLM).\n",
    "  - Excellent for text understanding tasks such as sentiment analysis and document classification.\n",
    "  - Used by platforms like X (Twitter) for toxic content detection. <br/>\n",
    "<br/>\n",
    "- **GPT** (Decoder-only):\n",
    "  - Trained with next-token prediction.\n",
    "  - Designed for text generation tasks (translation, summarization, code generation, etc.).\n",
    "\n",
    "#### Learning Capabilities of GPT\n",
    "> Zero-shot learning: Solving new tasks without seeing any examples.<br/>\n",
    "> Few-shot learning: Learning from a very small number of examples.<br/>\n",
    "\n",
    "GPT models are originally trained for text completion, but show high versatility.\n",
    "\n",
    "**Key difference:**  \n",
    "BERT is optimized for *understanding*, while GPT is optimized for *generation*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2408d7c8-906b-4cbe-95cb-a81d292aa96f",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; margin-top: 20px;\">\n",
    "  <img \n",
    "    src=\"https://raw.githubusercontent.com/salavii/llm-from-scratch/main/images/GPT..png\"\n",
    "    style=\"width: 600px; border-radius: 10px; display: block; margin-left: auto; margin-right: auto;\"\n",
    "  >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9da2db-7b5e-4f41-a006-ffc8f793d187",
   "metadata": {},
   "source": [
    "## Transformers vs. LLMs\n",
    "Most modern LLMs are based on the transformer architecture. Therefore, the terms Transformer and LLM are often used interchangeably.\n",
    "\n",
    "**Not all transformers are LLMs.** <BR/>\n",
    "- Transformers are also used in computer vision.\n",
    "\n",
    "**Not all LLMs are transformers.** <BR/>\n",
    "Some LLMs are based on:<BR/>\n",
    "- Recurrent architectures (RNNs)\n",
    "- Convolutional architectures (CNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437cbfaa-19b2-458a-9fae-50573059aeb3",
   "metadata": {},
   "source": [
    "### Building a Large Language Model\n",
    "\n",
    "- After introducing the core concepts behind LLMs, we now move on to **implementing one from scratch**.\n",
    "- We will use the **fundamental idea behind GPT** as a **blueprint**.\n",
    "- The implementation will be structured into **three stages**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ac476f-eda6-40b5-940b-ed1118e3d3aa",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; margin-top: 20px;\">\n",
    "  <img \n",
    "    src=\"https://raw.githubusercontent.com/salavii/llm-from-scratch/main/images/model.png\"\n",
    "    style=\"width: 850px; border-radius: 10px; display: block; margin-left: auto; margin-right: auto;\"\n",
    "  >\n",
    "\n",
    "  <p style=\"font-size: 16px; color: #333; font-weight: bold; margin-top: 10px;\">\n",
    "    The three main stages of coding an LLM are implementing the LLM architecture and data preparation \n",
    "    process (stage 1), pretraining an LLM to create a foundation model (stage 2), and fine-tuning the foundation \n",
    "    model to become a personal assistant or text classifier (stage 3).\n",
    "  </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3039951b-fe4c-4c5d-8444-2bbd74b995c3",
   "metadata": {},
   "source": [
    "### In stage 1 \n",
    "We will learn about the fundamental data preprocessing steps and code the attention mechanism at the heart of every LLM.\n",
    "\n",
    "### In stage 2\n",
    "We will learn how to code and pretrain a GPT-like LLM capable of generating new texts. We will also go\n",
    "over the fundamentals of evaluating LLMs, which is essential for developing capable\n",
    "NLP systems. \n",
    "\n",
    "### In stage 3 \n",
    "We will take a pretrained LLM and fine-tune it to follow instructions such as answering queries or classifying texts—the most common tasks in many real-world applications and research."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
