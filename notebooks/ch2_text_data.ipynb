{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c97d942f-1b6f-487a-bd0e-be37ae100ad7",
   "metadata": {},
   "source": [
    "# $$ Text-Data $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e4a332-b058-4cb5-9382-5cf31cf8dbde",
   "metadata": {},
   "source": [
    "**Our focus is on step 1 of stage 1: implementing the data sampling pipeline.**\n",
    "\n",
    "### Pretraining Insight:\n",
    "✅ Process text `one word` at a time <br/>\n",
    "✅ Are trained on next-word prediction<br/>\n",
    "- Models with:<br/>\n",
    "\n",
    "✅ Millions to billions of parameters<br/>\n",
    "→ Achieve impressive capabilities\n",
    "\n",
    "### Why Data Preparation Is Needed\n",
    "Before training an LLM:\n",
    "\n",
    "✅ The training dataset must be prepared <br/>\n",
    "Data must be:\n",
    "\n",
    "- Tokenized\n",
    "- Vectorized\n",
    "- Sampled correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f135d5-3c12-4c38-84d3-5798bcac3918",
   "metadata": {},
   "source": [
    "### What Is an Embedding?\n",
    "Word embeddings convert words into numerical vectors that preserve semantic meaning.\n",
    "\n",
    "### Why Embeddings Are Needed\n",
    "- Neural networks cannot process raw text directly.\n",
    "- Text is categorical, not numerical.\n",
    "- Neural networks require:\n",
    "✅ Continuous-valued numerical vectors.\n",
    "\n",
    "### Embedding Models\n",
    "Embeddings can be created using:\n",
    "- A neural network layer.\n",
    "- A pretrained embedding model\n",
    "\n",
    "Different data types require: <br/>\n",
    "✅ Different embedding models. <br/>\n",
    "Text ≠ Audio ≠ Video\n",
    "\n",
    "### Types of Text Embeddings\n",
    "- Sentences\n",
    "- Paragraphs\n",
    "- Entire documents\n",
    "\n",
    "Sentence and paragraph embeddings are commonly used in Retrieval-Augmented Generation (RAG) systems, which combine:\n",
    "- Generation (text creation)\n",
    "- Retrieval (searching external knowledge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bd238c-8188-42eb-95b3-2f5c521f5306",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "**One of the earliest and most popular word embedding methods is `Word2Vec`.**\n",
    "\n",
    "How it works: <br/>\n",
    "It trains a neural network to:\n",
    "- Predict the context from a word.\n",
    "- Or predict the word from its context.\n",
    "\n",
    "**Main Idea:**\n",
    "\n",
    "Words that appear in similar contexts tend to have similar meanings.\n",
    "\n",
    "As a result, when visualized in 2D space:\n",
    "\n",
    "> Similar words appear close together <br/>\n",
    "> Related terms form clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a3aea7-15af-498c-af02-89d37113a496",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; margin-top: 20px;\">\n",
    "  <img \n",
    "    src=\"https://raw.githubusercontent.com/salavii/llm-from-scratch/main/images/2D Visualization of Word Embedding Space.png\"\n",
    "    style=\"width: 750px; border-radius: 10px; display: block; margin-left: auto; margin-right: auto;\"\n",
    "  >\n",
    "\n",
    "  <p style=\"font-size: 16px; color: #333; font-weight: bold; margin-top: 10px;\">\n",
    "    This figure visualizes word embeddings projected into a two-dimensional space, where semantically similar words appear closer together. <br/>\n",
    "    It demonstrates how relationships between concepts like animals, locations, and adjectives emerge in embedding space\n",
    "  </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae34bd23-81c3-43d4-ae44-188c2ad8a3c0",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0f2177-af14-4610-9f44-c1fc1c0f8509",
   "metadata": {},
   "source": [
    "## Embeddings in LLMs vs Word2Vec\n",
    "In classical ML, embeddings can be generated using pretrained models such as Word2Vec.\n",
    "\n",
    "In LLMs, embeddings are part of the model itself and are:\n",
    "- Learned from scratch\n",
    "- Updated during training\n",
    "\n",
    "**Advantage:** LLM embeddings are optimized for:\n",
    "- The specific task\n",
    "- The specific dataset\n",
    "\n",
    "**Real LLM embeddings are high-dimensional (hundreds to thousands)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866623ab-8f13-4581-8c90-082041e0c5fd",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; margin-top: 20px;\">\n",
    "  <img \n",
    "    src=\"https://raw.githubusercontent.com/salavii/llm-from-scratch/main/images/Text Tokenization to GPT Input Pipeline.png\"\n",
    "    style=\"width: 750px; border-radius: 10px; display: block; margin-left: auto; margin-right: auto;\"\n",
    "  >\n",
    "\n",
    "  <p style=\"font-size: 16px; color: #333; font-weight: bold; margin-top: 10px;\">\n",
    "    This diagram illustrates the full pipeline from raw input text to token IDs and embeddings fed into a GPT-like decoder-only transformer. <br/>\n",
    "    It shows how tokenization, embedding lookup, and postprocessing connect input text to generated output\n",
    "  </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b1900c-45ad-449c-b9a3-068d90d0d46f",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9687afbc-2a50-4f02-b9f7-baa41beb1535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the module for making HTTP requests and downloading files\n",
    "import urllib.request\n",
    "url = (\"https://raw.githubusercontent.com/salavii/llm-from-scratch/refs/heads/main/data/the-verdict.txt\")\n",
    "file_path = \"the-verdict.txt\"\n",
    "\n",
    "urllib.request.urlretrieve(url, file_path) # Download the file from the given URL and store it as \"the-verdict.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5acde05-6339-4e90-aae0-622008687f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "# Open the text file in read mode using UTF-8 encoding\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f899bf4b-b460-4017-acc7-fbd6a82ba138",
   "metadata": {},
   "source": [
    "### Goal:\n",
    "Tokenize a 20,479-character short story into:\n",
    "- Individual words\n",
    "- Special characters\n",
    "\n",
    "So that these tokens can later be converted into `embeddings` for LLM training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9941a9a5-b724-4953-821c-30b8e77dc230",
   "metadata": {},
   "source": [
    "### How Do We Split the Text into Tokens?\n",
    "To illustrate the basic idea of splitting text into tokens, we:\n",
    "- Use Python’s **re (regular expression)** library.\n",
    "\n",
    "Apply re.split to:\n",
    "- Split text based on whitespace\n",
    "\n",
    "This is only for demonstration.\n",
    "Later, we will switch to a prebuilt tokenizer, and no regex knowledge will be required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32bc57f8-a2c4-4766-9dbc-a25516434296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "\n",
    "result = re.split(r'(\\s)', text)        # Split the text based on whitespace characters\n",
    "                                        # Parentheses ensure that the spaces are also included in the output\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca00c2b-df26-48dd-8b24-b870a0722468",
   "metadata": {},
   "source": [
    "### Limitations of Simple Whitespace Tokenization\n",
    "A simple whitespace-based tokenization:\n",
    "\n",
    ">Separates most words correctly ✅ <br/>\n",
    ">But still leaves punctuation attached to words ❌"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e952b620-3621-499b-bd78-ef21f4f701a5",
   "metadata": {},
   "source": [
    "### We do not convert all text to lowercase because:\n",
    "Capitalization helps:\n",
    "- Distinguish $proper$ $nouns$ from $common$ $nouns$.\n",
    "- Understand sentence structure.\n",
    "- Learn correct text generation with proper capitalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d538f0a-e38c-4c22-833f-aea0ee680d1f",
   "metadata": {},
   "source": [
    "### Improved Tokenization Strategy\n",
    "\n",
    "To improve tokenization, we split the text based on:\n",
    "- Whitespace → \\s\n",
    "- Commas and periods → [,.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6ec5982-a102-4903-b13a-95d52ee55b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03766aa9-50a8-40ae-a46f-f644346869ab",
   "metadata": {},
   "source": [
    "A small remaining problem is that the list still includes whitespace characters. <br/>\n",
    "Optionally, we can remove these redundant characters safely as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f43443bb-f7cb-471a-8b3b-8e5167dbe099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]  # Remove whitespace-only tokens from the token list\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3766df-5ff3-421b-8d3f-2e4a0165747f",
   "metadata": {},
   "source": [
    "### Handling Whitespace in Tokenization\n",
    "When building a simple tokenizer, Whether to keep or remove whitespaces depends on the **application**.\n",
    "\n",
    "#### Removing whitespaces:\n",
    "- Reduces memory usage.\n",
    "- Improves computational efficiency.\n",
    "\n",
    "#### Keeping whitespaces:\n",
    "- Preserves exact text structure.\n",
    "- Is important for:\n",
    "  > Programming languages <br/>\n",
    "  > Indentation-sensitive data\n",
    "\n",
    "In this project:\n",
    "\n",
    "✅ Whitespaces are removed for simplicity. <br/>\n",
    "✅ A later tokenizer will include whitespaces again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9853fb0-1e82-4e22-8247-b4922b2d70e4",
   "metadata": {},
   "source": [
    "### Extending the Tokenization Scheme\n",
    "The current tokenizer works well for simple examples. <br/>\n",
    "However, real-world text also contains:\n",
    "- Question marks ?\n",
    "- Quotation marks \"\n",
    "- Double dashes --\n",
    "\n",
    "Other special characters\n",
    "\n",
    "Therefore:\n",
    "\n",
    "**The tokenization rules must be extended to correctly separate these symbols as well**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd6f3b2a-41f4-4109-961b-576fbce52bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22b13be-4714-47b3-86fd-53d1e169ec6a",
   "metadata": {},
   "source": [
    "Now that we have a basic tokenizer working, let’s apply it to Edith Wharton’s entire\n",
    "short story:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8c7a7e5-7a54-4a95-87f4-7b2682045904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488c41fa-8cd6-45a4-98d0-abf89ed9dd5d",
   "metadata": {},
   "source": [
    "# Converting tokens into token IDs\n",
    "After tokenization, tokens are still represented as:\n",
    "> Python strings (text)\n",
    "\n",
    "However, LLMs: <br/>\n",
    "Operate only on `numerical values.`\n",
    "\n",
    "Therefore:<br/>\n",
    "- Each token must be converted into a unique integer.\n",
    "- This integer is called a **Token ID.**\n",
    "\n",
    "This step is an $intermediate$ $stage$ before converting token IDs into:\n",
    "\n",
    "**Embedding vectors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c414d46c-5ca7-4fbb-b9bc-1aaf1231abd6",
   "metadata": {},
   "source": [
    "### Building the Vocabulary\n",
    "To map tokens to token IDs, we must first build a **vocabulary.**\n",
    "\n",
    "A vocabulary:\n",
    "- Contains all unique tokens\n",
    "- Assigns: <br/>\n",
    "> One unique integer To each word and special character."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d0f7bf-77af-4ff9-bb97-24a57e9c54a5",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b245c992-69d2-48d8-b75e-e2dd14472059",
   "metadata": {},
   "source": [
    "let’s create a list of all unique tokens and sort them\n",
    "alphabetically to determine the vocabulary size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0d9bc7f-27d5-48d7-9249-c12abd9c68dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size is: 1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(f'vocabulary size is: {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11905fae-b0c0-49ea-aa83-cee0f76072a6",
   "metadata": {},
   "source": [
    "Now, we create the vocabulary and print its first 51 entries for illustration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de775dd2-54a9-4862-bf7c-280d3a409073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "# Build the vocabulary by assigning a unique integer ID to each token\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "\n",
    "for i, item in enumerate(vocab.items()):      # Loop through the vocabulary items with an index counter\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3659f0-780b-46ef-8e84-51903fa4f232",
   "metadata": {},
   "source": [
    "## Converting Tokens to Token IDs using Vocabulary\n",
    "After tokenizing a new text, each token is mapped to a unique integer using an existing vocabulary. <br/>\n",
    "This vocabulary is built once from the full training dataset and is reused for all future text. <br/>\n",
    "As a result, any new sentence can be converted into a sequence of token IDs that the model can process. <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4273345b-4d6f-45dc-91fb-4cb4fd0098f0",
   "metadata": {},
   "source": [
    "## Encoding and Decoding with a Tokenizer\n",
    "When an LLM processes text, it operates on token IDs, not `raw strings`. <br/>\n",
    "We therefore need two directions of mapping:\n",
    "- **Encoding**\n",
    "- Text → Tokens → Token IDs\n",
    "- Implemented by an `encode` method\n",
    "- Uses a vocabulary that maps each token (string) to a unique integer ID\n",
    "\n",
    "- **Decoding**\n",
    "- Token IDs → Tokens → Text\n",
    "- Implemented by a `decode` method\n",
    "- Uses an inverse vocabulary that maps each integer ID back to its corresponding token\n",
    "\n",
    "By combining `encode` and `decode`, the tokenizer:\n",
    "- Converts human-readable text into numerical input for the model\n",
    "- Converts the model’s numerical output back into human-readable text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31e735cc-7df2-4201-8637-27da6bb0b69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode (self, text):\n",
    "        preprocessed = re.split (r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip() \n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode (self, ids):\n",
    "        text = \" \".join ([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,?.!\\\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cdceabe-c6bf-4ff0-b03f-f6a3c01a3917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "       Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667966ba-31a2-462d-b945-b551d35a8678",
   "metadata": {},
   "source": [
    "Next, let’s see whether we can turn these token IDs back into text using the decode\n",
    "method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8315dcde-5ab5-4a12-9b9a-9171217d65d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5d9a41-8ccb-49e6-bc4c-5b8fca547a77",
   "metadata": {},
   "source": [
    "So far, so good.<br/>\n",
    "Let’s now apply it to a new text sample not contained in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe56efe7-8f77-41f7-b83d-716dd7dbcfcf",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, do you like tea?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mencode(text))\n",
      "Cell \u001b[1;32mIn[9], line 11\u001b[0m, in \u001b[0;36mSimpleTokenizerV1.encode\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m      7\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit (\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m      8\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      9\u001b[0m     item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip() \n\u001b[0;32m     10\u001b[0m ]\n\u001b[1;32m---> 11\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Hello'"
     ]
    }
   ],
   "source": [
    "text = \"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3705d015-fe9c-4685-a6b8-4dc16036bf2a",
   "metadata": {},
   "source": [
    "The problem is that the word `Hello` was not used in the “The Verdict” short story. <br/>\n",
    "Hence, it is not contained in the vocabulary. This highlights the need to consider\n",
    "$large$ and $diverse$ training sets to extend the vocabulary when working on LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f009e598-a424-4281-90e4-f014011a4bd4",
   "metadata": {},
   "source": [
    "## Adding special context tokens\n",
    "In real-world scenarios, a tokenizer must be able to handle:\n",
    "- `Unknown words` that are not present in the vocabulary.\n",
    "- `Special context` tokens that provide additional information to the model.\n",
    "\n",
    "To address this, we extend the vocabulary and tokenizer to include two special tokens:\n",
    "- **<|unk|>** → Represents unknown tokens (out-of-vocabulary words)\n",
    "- **<|endoftext|>** → Marks the end of a document or text sequence\n",
    "\n",
    "We implement these changes in a new tokenizer version called SimpleTokenizerV2, which:\n",
    "> Maps any unseen word to <|unk|> <br/>\n",
    "> Optionally appends <|endoftext|> to indicate the end of the text\n",
    "\n",
    "These special tokens help the LLM:\n",
    "- Deal robustly with words it has not encountered during training\n",
    "- Better understand document boundaries and context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c152baaf-e2e7-4506-8159-8da9fdb1a1a3",
   "metadata": {},
   "source": [
    "## Using <|endoftext|> to Separate Independent Text Sources\n",
    "\n",
    "When training GPT-like large language models on multiple independent documents or texts, all sources are usually concatenated into a single long training sequence. To prevent the model from confusing unrelated texts, a special token called `<|endoftext|>` is inserted between each text.\n",
    "\n",
    "This token acts as a clear boundary marker that signals the end of one document and the start of another"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d91efbf-5e17-4c84-8da9-d6fc6dd367ee",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb02ea8-d2f4-4ded-bccb-6b39fcdaa633",
   "metadata": {},
   "source": [
    "Let’s now modify the vocabulary to include these two special tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b20c8e1f-0f40-4903-ba97-3c1391f4fcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate tokens, convert to list, and sort alphabetically\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "\n",
    "# Add special tokens for unknown words and document separation\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "# Create the vocabulary: map each token to a unique integer ID\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "print(len(vocab.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2756742-cc68-4c71-bb99-526da984724f",
   "metadata": {},
   "source": [
    " let’s print the last five entries of the updated vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cc4a2bb-b552-4f7e-a68b-21aa7f0f6edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79192ace-d660-4510-b642-4a920866bc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode (self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode (self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)   \n",
    "        return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e672fc21-9bbd-402d-a503-891da5fc51d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4af563-f2a7-435b-819e-742cc77ad982",
   "metadata": {},
   "source": [
    "Next, let’s tokenize the sample text using the SimpleTokenizerV2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46762c02-3086-403e-a953-af8486923e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "623d4376-54d0-49fd-a8d8-f961124220ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0b3165-5660-449b-ab5f-1bbac2c1bf82",
   "metadata": {},
   "source": [
    "# Byte pair encoding\n",
    "To move beyond simple word-level tokenization, we now explore a more advanced tokenization scheme called **Byte Pair Encoding (BPE)**. BPE-based tokenizers are used to train large language models such as`GPT-2`, `GPT-3`, and the original model behind ChatGPT.\n",
    "\n",
    "Implementing BPE from scratch can be relatively complex, so instead of reimplementing the algorithm, we use the open source Python library `tiktoken`. This library provides a highly efficient BPE tokenizer with a Rust-based backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c43c6918-1889-41da-be0f-c64bb13ff701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.12.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ae631a-33f9-4793-aec3-0db09417e983",
   "metadata": {},
   "source": [
    "we can instantiate the BPE tokenizer from tiktoken as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f1a62f9-251f-4bfe-b9bb-e67d63ea7750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer Loaded ✅\n"
     ]
    }
   ],
   "source": [
    "# Load GPT-2 compatible BPE tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(\"Tokenizer Loaded ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d92e08-71ea-46df-b1d9-4336e76b1751",
   "metadata": {},
   "source": [
    "The usage of this tokenizer is similar to the SimpleTokenizerV2 we implemented via an encode method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c608463b-383d-405a-be65-1bc667873399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "Integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(Integers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14af190b-8b3b-48a2-9528-8fb266e4f9c9",
   "metadata": {},
   "source": [
    "We can then convert the token IDs back into text using the decode method, similar to\n",
    "our SimpleTokenizerV2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b076773-13de-45be-8d74-0c01c0c6705d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "Strings = tokenizer.decode(Integers)\n",
    "print (Strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3892bf8-558e-4348-b7b1-edb891a053be",
   "metadata": {},
   "source": [
    "### Key Observations about BPE Tokenization\n",
    "1. **Vocabulary Size and `<|endoftext|>` Token**\n",
    "- The GPT-2 BPE tokenizer has a total vocabulary of **50,257 tokens**.\n",
    "- The token `<|endoftext|>` is assigned the **largest token ID: 50256**.\n",
    "- This token is mainly used to mark **the end of a document or text sequence.**\n",
    "\n",
    "2. **No Need for `<|unk|>` in BPE**\n",
    "- Unlike simple tokenizers, **BPE does not require an `<|unk|>` (unknown) token.**\n",
    "- When the tokenizer encounters an unknown word, it:\n",
    "    - Splits it into **subword units**\n",
    "    - Or even into **individual characters**\n",
    "    - This guarantees that **any word can always be represented**\n",
    "\n",
    "3. **Why This is Important for LLMs** <br/>\n",
    "The model can handle:\n",
    "- New words\n",
    "- Names\n",
    "- Technical terms\n",
    "- Misspellings\n",
    "- Without ever failing due to “unknown vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6509238e-15d5-43e4-b8f9-cfffc5c0b1db",
   "metadata": {},
   "source": [
    "### Byte pair encoding of unknown words \n",
    "Try the BPE tokenizer from the tiktoken library on the unknown words “Akwirw ier” and\n",
    "print the individual token IDs. Then, call the decode function on each of the resulting\n",
    "integers in this list. Lastly, call the\n",
    "decode method on the token IDs to check whether it can reconstruct the original\n",
    "input, “Akwirw ier.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8618f27-5fa2-4092-a037-35aa08ddfe86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n"
     ]
    }
   ],
   "source": [
    "TEXT = \"Akwirw ier\"\n",
    "int = tokenizer.encode(TEXT, allowed_special={\"<|endoftext|>\"})\n",
    "print (int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29838185-3b6d-4d38-8433-748cb54c3b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33901 : Ak\n",
      "86 : w\n",
      "343 : ir\n",
      "86 : w\n",
      "220 :  \n",
      "959 : ier\n"
     ]
    }
   ],
   "source": [
    "for i in int:\n",
    "    print (f\"{i} : {tokenizer.decode([i])}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4432efb5-dba2-4457-b7a7-865ae489d67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "# str = tokenizer.decode([86,220,343])\n",
    "\n",
    "str = tokenizer.decode(int)\n",
    "print (str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d790fee-f48c-4a4f-92af-183209828299",
   "metadata": {},
   "source": [
    "# Data Sampling with Sliding Window\n",
    "During LLM training, the text is converted into `input–target` pairs, where the model sees a sequence of tokens as input and learns to **predict the next token as the target.** <br/>\n",
    "To generate these training pairs, a `**sliding window**` approach is used. This moving window scans across the text step by step and continuously creates overlapping training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11dd534-9761-4921-a9f8-44dbd49b4e11",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; margin-top: 20px;\">\n",
    "  <img \n",
    "    src=\"https://raw.githubusercontent.com/salavii/llm-from-scratch/main/images/sliding window.png\"\n",
    "    style=\"width: 750px; border-radius: 10px; display: block; margin-left: auto; margin-right: auto;\"\n",
    "  >\n",
    "\n",
    "  <p style=\"font-size: 16px; color: #333; font-weight: bold; margin-top: 10px;\">\n",
    "   This figure shows how Large Language Models (LLMs) are trained to predict one word at a time using a next-token prediction  task.\n",
    "  </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e303dc6-fd33-4564-b95e-64123c305611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d1c817-08b4-4aa7-bafb-015afb65342f",
   "metadata": {},
   "source": [
    "Next, we remove the first 50 tokens from the dataset for demonstration purposes,\n",
    "as it results in a slightly more interesting text passage in the next steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50b0cba0-5957-451f-bdf5-e1880c51d027",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f288bd-6fe2-4a3c-ab95-e220efc13253",
   "metadata": {},
   "source": [
    "### Creating Input–Target Pairs (x and y)\n",
    "A simple and intuitive way to build training data for next-token prediction is to create two variables:\n",
    "\n",
    "- x → the input token sequence\n",
    "- y → the target token sequence (x shifted by one position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a7b5545-5781-4a27-935d-579097c34ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [40, 367, 2885, 1464]\n",
      "y:      [367, 2885, 1464, 1807]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "x = enc_text[:context_size]\n",
    "y = enc_text[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36c27d68-fbca-4206-a8c0-8fab5becabe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] -----> 4920\n",
      "[290, 4920] -----> 2241\n",
      "[290, 4920, 2241] -----> 287\n",
      "[290, 4920, 2241, 287] -----> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"----->\", desired)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bea362-8ae7-40a3-9ef5-fca0ce6a751f",
   "metadata": {},
   "source": [
    "Let’s repeat the previous code but convert the token IDs\n",
    "into text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17bc438c-1547-4dc0-a574-3a56310d65bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ----->  established\n",
      " and established ----->  himself\n",
      " and established himself ----->  in\n",
      " and established himself in ----->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desire = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"----->\", tokenizer.decode([desire]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaf7140-4071-4f89-8744-c64bb2cead07",
   "metadata": {},
   "source": [
    "## Preparing Tensors with a Data Loader\n",
    "At this stage, we have already constructed the **input–target** pairs required for LLM training. <br/>\n",
    "Before converting tokens into embeddings, one final step remains: implementing an **efficient data loader**.\n",
    "\n",
    "The `data loader` iterates over the tokenized dataset and returns:\n",
    "- an **input tensor** containing the token sequences seen by the LLM,\n",
    "- a **target tensor** containing the next-token labels that the model must predict.\n",
    "\n",
    "These tensors are returned in the form of **PyTorch tensors**, which are multidimensional numerical arrays used for deep learning computations.\n",
    "\n",
    "Although tokens are shown as text strings in illustrations, the actual implementation operates **directly on token IDs**, since the BPE tokenizer’s `encode` method performs both tokenization and ID conversion in a single step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b810fa5-44fe-42a2-a8d3-234df2a365ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c7ba7e2-d53b-405e-818e-501917874fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenizes the entire text\n",
    "        token_ids = tokenizer.encode(text)\n",
    "\n",
    "        # Uses a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    # Returns the total number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    # Returns a single row from the dataset\n",
    "    def __getitem__(self, idx):        \n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6704c0e1-9882-40a6-b2e1-061558dc1069",
   "metadata": {},
   "source": [
    "#### This dataset is later combined with a PyTorch DataLoader, which:\n",
    "- Groups samples into batches\n",
    "- Enables efficient iteration during training\n",
    "- Improves performance and memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56441fdf-bbb8-42b8-a2ef-959ba116ad97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDataLoader_V1(text, batch_size=4, max_length=256, stride=128, \n",
    "                        shuffle=True, drop_last=True, num_workers=0):\n",
    "\n",
    "    token_ids = tokenizer.encode(text)\n",
    "\n",
    "    dataset = GPTDatasetV1(text, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(dataset, \n",
    "                            batch_size= batch_size,\n",
    "                            shuffle= shuffle,\n",
    "                            drop_last= drop_last,\n",
    "                            num_workers= num_workers\n",
    "                           )\n",
    "    return dataloader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b6d5269a-779c-4878-ae0a-c931f585cb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "dataloader = CreateDataLoader_V1( raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader)     \n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a5615f5-b2a5-43a0-886f-d971c1160db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6548ecd1-a64a-4e5d-9989-6669207afd8a",
   "metadata": {},
   "source": [
    "**Exercise 2.2 Data loaders with different strides and context sizes.<br/>**\n",
    "To develop more intuition for how the data loader works, try to run it with different\n",
    "settings such as `max_length=2 and stride=2`, and `max_length=8 and stride=2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "78e630ee-869f-4c6c-8cf7-47700deca8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 40, 367]]), tensor([[ 367, 2885]])]\n"
     ]
    }
   ],
   "source": [
    "dataloader_E = CreateDataLoader_V1(raw_text, batch_size=1, max_length= 2, stride=2, shuffle= False)\n",
    "data_iter_E = iter(dataloader_E)\n",
    "first_batch_E = next(data_iter_E)\n",
    "print(first_batch_E)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1f87ce-3d7d-472b-9f4f-8fe4709e9560",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; margin-top: 20px;\">\n",
    "  <img \n",
    "    src=\"https://raw.githubusercontent.com/salavii/llm-from-scratch/main/images/stride.png\"\n",
    "    style=\"width: 650px; border-radius: 10px; display: block; margin-left: auto; margin-right: auto;\"\n",
    "  >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b97b4f1-16e4-41fd-933a-9de47909fb75",
   "metadata": {},
   "source": [
    "Let’s look briefly at how we can use the data loader to sample with a batch size\n",
    "greater than 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f61bc526-8f16-4140-8359-2ea37e49c8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input is:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]]) \n",
      "\n",
      " target is:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = CreateDataLoader_V1(raw_text, batch_size=8, max_length=4, stride=4, shuffle= False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(f\"input is:\\n {inputs} \\n\\n target is:\\n {targets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5cacd1-81a2-41c7-9de5-baf00ca4f6fc",
   "metadata": {},
   "source": [
    "Increasing the stride to `4` ensures that we fully utilize the dataset without skipping any tokens.\n",
    "\n",
    "Because the stride equals the context length, the extracted sequences do not overlap.\n",
    "This eliminates redundant training samples and reduces the risk of overfitting, since the model does not repeatedly see nearly identical input windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8160e70-7e72-4bd6-822d-4fde36b23d04",
   "metadata": {},
   "source": [
    "# Creating token embeddings\n",
    "To train an LLM, token IDs must be converted into continuous numerical vectors called **embeddings.** <be/>\n",
    "Neural networks cannot learn semantic meaning from integers alone, so an embedding layer maps each token ID to a dense vector.\n",
    "\n",
    "### Why embeddings are needed:\n",
    "- Token IDs (e.g., `2`, `15`, `501`) carry no meaning by themselves.\n",
    "- Embeddings transform these IDs into vectors that capture relationships between words.\n",
    "- During training, **backpropagation** updates these vectors so the model learns semantics (e.g., cat and dog become closer in vector space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24c7754-341b-4b8a-a949-29d3a60530f7",
   "metadata": {},
   "source": [
    "Let’s see how the token ID to embedding vector conversion works with a hands-on\n",
    "example. Suppose we have the following four input tokens with IDs 2, 3, 5, and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ea66a387-c9b1-4bb6-8e44-567a51c49d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09e1e6c-b96b-448e-9872-3da15655d2b6",
   "metadata": {},
   "source": [
    " suppose we have a small vocabulary of only 6 words (instead\n",
    "of the 50,257 words in the BPE tokenizer vocabulary), and we want to create embed\n",
    "dings of size 3 (in GPT-3, the embedding size is 12,288 dimensions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d0bef91-64d4-481d-acd1-677c0ed63e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0540338-af5e-42fb-a9c0-312c422f091e",
   "metadata": {},
   "source": [
    "Using the `vocab_size` and `output_dim`, **we can instantiate an embedding layer in\n",
    "PyTorch, setting the random seed to 123 for reproducibility purposes:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "172f4155-c95c-48c5-9f54-0d8837bb350b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749d1319-9e83-4a13-9060-998fe8c730e2",
   "metadata": {},
   "source": [
    "When we create an embedding layer in PyTorch, it generates a weight matrix that stores the embedding vectors for all tokens.\n",
    "\n",
    "Structure of the embedding matrix\n",
    "\n",
    "Shape: [vocab_size, embedding_dim]\n",
    "\n",
    "One row per token ID\n",
    "\n",
    "One column per embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "25a8b0ef-cbf8-41b7-b2fe-0d57b187eabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "73725589-0fb5-405f-916f-ac399199b15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5d0366-3d87-4862-9611-771a439554af",
   "metadata": {},
   "source": [
    "## Next Step: Adding Positional Information\n",
    "\n",
    "While embeddings capture the meaning of tokens,\n",
    "**they do not encode the order of tokens in the sequence.**\n",
    "\n",
    "To allow the model to understand sentence structure,\n",
    "we must add **positional encodings** to the embeddings.\n",
    "\n",
    "This enables the LLM to distinguish between differently ordered sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a0cddc-6173-4178-b81a-d8f86514e08e",
   "metadata": {},
   "source": [
    "## Encoding word positions\n",
    "Why Embeddings Alone Are Not Enough\n",
    "\n",
    "While token embeddings provide a continuous vector representation of each token,\n",
    "they do **not** encode the position of tokens in the sequence.\n",
    "\n",
    "Self-attention does not inherently understand the order of tokens.\n",
    "A token ID always maps to the same embedding vector, regardless of whether it\n",
    "appears at the beginning, middle, or end of a sentence.\n",
    "\n",
    "**Result:**\n",
    "The model cannot distinguish between sequences with the same words but different order.\n",
    "\n",
    "To fix this, we need **positional** encodings, which provide information about\n",
    "the position of each token in the input sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a633bfe-821a-4819-8fda-21680fd9d444",
   "metadata": {},
   "source": [
    "## Relative vs. Absolute Positional Embeddings\n",
    "LLMs need positional information because self-attention does not inherently understand token order.\n",
    "\n",
    "#### Relative Positional Embeddings\n",
    "These embeddings focus on **distances between tokens** (“how far apart”) rather than absolute positions. <br/>\n",
    "They help the model generalize better to sequences of different lengths, even ones unseen during training.\n",
    "\n",
    "#### Absolute Positional Embeddings\n",
    "Each position (0, 1, 2, …) has its own learnable embedding vector.<br/>\n",
    "`GPT models` use this type and optimize these embeddings during training.\n",
    "\n",
    "#### Why this matters\n",
    "Both methods enrich the LLM with knowledge of ordering and token relationships, enabling more coherent and context-aware predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e44f84-4db5-46c2-96c4-ad7b2b1fad8f",
   "metadata": {},
   "source": [
    "### Increasing the Embedding Size\n",
    "For realistic LLM inputs, token IDs are mapped into high-dimensional embedding vectors.\n",
    "Here, we use 256 dimensions—much smaller than GPT-3’s 12,288, but sufficient for experimentation.\n",
    "\n",
    "The BPE tokenizer used earlier provides a vocabulary size of 50,257, which defines the embedding matrix shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1acea542-855e-4f95-b1bc-aa03dd72ea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cc274090-18a7-4db8-a080-33ece531fbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: \n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]]) \n",
      "\n",
      " Inputs shape: \n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "dataloader = CreateDataLoader_V1(\n",
    "                                raw_text, batch_size= 8,\n",
    "                                max_length= max_length, \n",
    "                                stride= max_length, shuffle= False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print (f\"Token IDs: \\n {inputs} \\n\\n Inputs shape: \\n {inputs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0121576e-3490-46d8-a9d4-33ba5862d192",
   "metadata": {},
   "source": [
    " Let’s now use the embedding layer to embed these token IDs into 256-dimensional\n",
    "vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6e62cecf-c18b-462b-b61f-7301a085870d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print (token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e936e8db-f4a0-42be-add4-40b79a79ed67",
   "metadata": {},
   "source": [
    "The 8 × 4 × 256–dimensional tensor output shows that each token ID is now embed\n",
    "ded as a 256-dimensional vector.\n",
    " For a GPT model’s absolute embedding approach, we just need to create another\n",
    "embedding layer that has the same embedding dimension as the token_embedding_\n",
    "layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "94ba4368-3961-45c0-a7d8-81edfe75c717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_lenght = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_lenght, output_dim)\n",
    "pos_embeddings = poss_embedding_layer(torch.arange(context_lenght))\n",
    "print(poss_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f06e2793-3946-433d-a3d9-7a51a1258561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a0e7d5-f7e4-48d6-b01b-f12f99febe27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
