{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c97d942f-1b6f-487a-bd0e-be37ae100ad7",
   "metadata": {},
   "source": [
    "# $$ Text-Data $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e4a332-b058-4cb5-9382-5cf31cf8dbde",
   "metadata": {},
   "source": [
    "**Our focus is on step 1 of stage 1: implementing the data sampling pipeline.**\n",
    "\n",
    "### Pretraining Insight:\n",
    "✅ Process text `one word` at a time <br/>\n",
    "✅ Are trained on next-word prediction<br/>\n",
    "- Models with:<br/>\n",
    "\n",
    "✅ Millions to billions of parameters<br/>\n",
    "→ Achieve impressive capabilities\n",
    "\n",
    "### Why Data Preparation Is Needed\n",
    "Before training an LLM:\n",
    "\n",
    "✅ The training dataset must be prepared <br/>\n",
    "Data must be:\n",
    "\n",
    "- Tokenized\n",
    "- Vectorized\n",
    "- Sampled correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f135d5-3c12-4c38-84d3-5798bcac3918",
   "metadata": {},
   "source": [
    "### What Is an Embedding?\n",
    "Word embeddings convert words into numerical vectors that preserve semantic meaning.\n",
    "\n",
    "### Why Embeddings Are Needed\n",
    "- Neural networks cannot process raw text directly.\n",
    "- Text is categorical, not numerical.\n",
    "- Neural networks require:\n",
    "✅ Continuous-valued numerical vectors.\n",
    "\n",
    "### Embedding Models\n",
    "Embeddings can be created using:\n",
    "- A neural network layer.\n",
    "- A pretrained embedding model\n",
    "\n",
    "Different data types require: <br/>\n",
    "✅ Different embedding models. <br/>\n",
    "Text ≠ Audio ≠ Video\n",
    "\n",
    "### Types of Text Embeddings\n",
    "- Sentences\n",
    "- Paragraphs\n",
    "- Entire documents\n",
    "\n",
    "Sentence and paragraph embeddings are commonly used in Retrieval-Augmented Generation (RAG) systems, which combine:\n",
    "- Generation (text creation)\n",
    "- Retrieval (searching external knowledge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bd238c-8188-42eb-95b3-2f5c521f5306",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "**One of the earliest and most popular word embedding methods is `Word2Vec`.**\n",
    "\n",
    "How it works: <br/>\n",
    "It trains a neural network to:\n",
    "- Predict the context from a word.\n",
    "- Or predict the word from its context.\n",
    "\n",
    "**Main Idea:**\n",
    "\n",
    "Words that appear in similar contexts tend to have similar meanings.\n",
    "\n",
    "As a result, when visualized in 2D space:\n",
    "\n",
    "> Similar words appear close together <br/>\n",
    "> Related terms form clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a3aea7-15af-498c-af02-89d37113a496",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; margin-top: 20px;\">\n",
    "  <img \n",
    "    src=\"https://raw.githubusercontent.com/salavii/llm-from-scratch/main/images/2D Visualization of Word Embedding Space.png\"\n",
    "    style=\"width: 750px; border-radius: 10px; display: block; margin-left: auto; margin-right: auto;\"\n",
    "  >\n",
    "\n",
    "  <p style=\"font-size: 16px; color: #333; font-weight: bold; margin-top: 10px;\">\n",
    "    This figure visualizes word embeddings projected into a two-dimensional space, where semantically similar words appear closer together. <br/>\n",
    "    It demonstrates how relationships between concepts like animals, locations, and adjectives emerge in embedding space\n",
    "  </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae34bd23-81c3-43d4-ae44-188c2ad8a3c0",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0f2177-af14-4610-9f44-c1fc1c0f8509",
   "metadata": {},
   "source": [
    "## Embeddings in LLMs vs Word2Vec\n",
    "In classical ML, embeddings can be generated using pretrained models such as Word2Vec.\n",
    "\n",
    "In LLMs, embeddings are part of the model itself and are:\n",
    "- Learned from scratch\n",
    "- Updated during training\n",
    "\n",
    "**Advantage:** LLM embeddings are optimized for:\n",
    "- The specific task\n",
    "- The specific dataset\n",
    "\n",
    "**Real LLM embeddings are high-dimensional (hundreds to thousands)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866623ab-8f13-4581-8c90-082041e0c5fd",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; margin-top: 20px;\">\n",
    "  <img \n",
    "    src=\"https://raw.githubusercontent.com/salavii/llm-from-scratch/main/images/Text Tokenization to GPT Input Pipeline.png\"\n",
    "    style=\"width: 750px; border-radius: 10px; display: block; margin-left: auto; margin-right: auto;\"\n",
    "  >\n",
    "\n",
    "  <p style=\"font-size: 16px; color: #333; font-weight: bold; margin-top: 10px;\">\n",
    "    This diagram illustrates the full pipeline from raw input text to token IDs and embeddings fed into a GPT-like decoder-only transformer. <br/>\n",
    "    It shows how tokenization, embedding lookup, and postprocessing connect input text to generated output\n",
    "  </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b1900c-45ad-449c-b9a3-068d90d0d46f",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9687afbc-2a50-4f02-b9f7-baa41beb1535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the-verdict.txt', <http.client.HTTPMessage at 0x22afb284b90>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the module for making HTTP requests and downloading files\n",
    "import urllib.request\n",
    "url = (\"https://raw.githubusercontent.com/salavii/llm-from-scratch/refs/heads/main/data/the-verdict.txt\")\n",
    "file_path = \"the-verdict.txt\"\n",
    "\n",
    "urllib.request.urlretrieve(url, file_path) # Download the file from the given URL and store it as \"the-verdict.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5acde05-6339-4e90-aae0-622008687f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "# Open the text file in read mode using UTF-8 encoding\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f899bf4b-b460-4017-acc7-fbd6a82ba138",
   "metadata": {},
   "source": [
    "### Goal:\n",
    "Tokenize a 20,479-character short story into:\n",
    "- Individual words\n",
    "- Special characters\n",
    "\n",
    "So that these tokens can later be converted into `embeddings` for LLM training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9941a9a5-b724-4953-821c-30b8e77dc230",
   "metadata": {},
   "source": [
    "### How Do We Split the Text into Tokens?\n",
    "To illustrate the basic idea of splitting text into tokens, we:\n",
    "- Use Python’s **re (regular expression)** library.\n",
    "\n",
    "Apply re.split to:\n",
    "- Split text based on whitespace\n",
    "\n",
    "This is only for demonstration.\n",
    "Later, we will switch to a prebuilt tokenizer, and no regex knowledge will be required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32bc57f8-a2c4-4766-9dbc-a25516434296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "\n",
    "result = re.split(r'(\\s)', text)        # Split the text based on whitespace characters\n",
    "                                        # Parentheses ensure that the spaces are also included in the output\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca00c2b-df26-48dd-8b24-b870a0722468",
   "metadata": {},
   "source": [
    "### Limitations of Simple Whitespace Tokenization\n",
    "A simple whitespace-based tokenization:\n",
    "\n",
    ">Separates most words correctly ✅ <br/>\n",
    ">But still leaves punctuation attached to words ❌"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e952b620-3621-499b-bd78-ef21f4f701a5",
   "metadata": {},
   "source": [
    "### We do not convert all text to lowercase because:\n",
    "Capitalization helps:\n",
    "- Distinguish $proper$ $nouns$ from $common$ $nouns$.\n",
    "- Understand sentence structure.\n",
    "- Learn correct text generation with proper capitalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d538f0a-e38c-4c22-833f-aea0ee680d1f",
   "metadata": {},
   "source": [
    "### Improved Tokenization Strategy\n",
    "\n",
    "To improve tokenization, we split the text based on:\n",
    "- Whitespace → \\s\n",
    "- Commas and periods → [,.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6ec5982-a102-4903-b13a-95d52ee55b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03766aa9-50a8-40ae-a46f-f644346869ab",
   "metadata": {},
   "source": [
    "A small remaining problem is that the list still includes whitespace characters. <br/>\n",
    "Optionally, we can remove these redundant characters safely as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f43443bb-f7cb-471a-8b3b-8e5167dbe099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]  # Remove whitespace-only tokens from the token list\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3766df-5ff3-421b-8d3f-2e4a0165747f",
   "metadata": {},
   "source": [
    "### Handling Whitespace in Tokenization\n",
    "When building a simple tokenizer, Whether to keep or remove whitespaces depends on the **application**.\n",
    "\n",
    "#### Removing whitespaces:\n",
    "- Reduces memory usage.\n",
    "- Improves computational efficiency.\n",
    "\n",
    "#### Keeping whitespaces:\n",
    "- Preserves exact text structure.\n",
    "- Is important for:\n",
    "  > Programming languages <br/>\n",
    "  > Indentation-sensitive data\n",
    "\n",
    "In this project:\n",
    "\n",
    "✅ Whitespaces are removed for simplicity. <br/>\n",
    "✅ A later tokenizer will include whitespaces again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9853fb0-1e82-4e22-8247-b4922b2d70e4",
   "metadata": {},
   "source": [
    "### Extending the Tokenization Scheme\n",
    "The current tokenizer works well for simple examples. <br/>\n",
    "However, real-world text also contains:\n",
    "- Question marks ?\n",
    "- Quotation marks \"\n",
    "- Double dashes --\n",
    "\n",
    "Other special characters\n",
    "\n",
    "Therefore:\n",
    "\n",
    "**The tokenization rules must be extended to correctly separate these symbols as well**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd6f3b2a-41f4-4109-961b-576fbce52bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22b13be-4714-47b3-86fd-53d1e169ec6a",
   "metadata": {},
   "source": [
    "Now that we have a basic tokenizer working, let’s apply it to Edith Wharton’s entire\n",
    "short story:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8c7a7e5-7a54-4a95-87f4-7b2682045904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488c41fa-8cd6-45a4-98d0-abf89ed9dd5d",
   "metadata": {},
   "source": [
    "# Converting tokens into token IDs\n",
    "After tokenization, tokens are still represented as:\n",
    "> Python strings (text)\n",
    "\n",
    "However, LLMs: <br/>\n",
    "Operate only on `numerical values.`\n",
    "\n",
    "Therefore:<br/>\n",
    "- Each token must be converted into a unique integer.\n",
    "- This integer is called a **Token ID.**\n",
    "\n",
    "This step is an $intermediate$ $stage$ before converting token IDs into:\n",
    "\n",
    "**Embedding vectors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c414d46c-5ca7-4fbb-b9bc-1aaf1231abd6",
   "metadata": {},
   "source": [
    "### Building the Vocabulary\n",
    "To map tokens to token IDs, we must first build a **vocabulary.**\n",
    "\n",
    "A vocabulary:\n",
    "- Contains all unique tokens\n",
    "- Assigns: <br/>\n",
    "> One unique integer To each word and special character."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d0f7bf-77af-4ff9-bb97-24a57e9c54a5",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b245c992-69d2-48d8-b75e-e2dd14472059",
   "metadata": {},
   "source": [
    "let’s create a list of all unique tokens and sort them\n",
    "alphabetically to determine the vocabulary size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0d9bc7f-27d5-48d7-9249-c12abd9c68dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size is: 1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(f'vocabulary size is: {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11905fae-b0c0-49ea-aa83-cee0f76072a6",
   "metadata": {},
   "source": [
    "Now, we create the vocabulary and print its first 51 entries for illustration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de775dd2-54a9-4862-bf7c-280d3a409073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ee80cd-5c70-4510-a0de-b06627edf62b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
